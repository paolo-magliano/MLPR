\documentclass{article}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[vmargin=1in, hmargin=1.5in]{geometry}

% Title and author information
\title{Machine Learning and Pattern Recognition Project Report}
\author{Paolo Magliano s314867}

\begin{document}

\maketitle

\begin{abstract}
This report describes the work done on Machine Learning and Pattern Recognition project. The report follows the structure of course laboratories, which studies and analyses the dataset and the models:
\begin{itemize}
    \item Multivariate Gaussian Model
    \item Logistic Regression
    \item Support Vector Machines
    \item Gaussian Mixture Model
\end{itemize}
\end{abstract}

% \tableofcontents

\section{Lab 2: Dataset}
\label{sec:dataset}
The samples are computed by a feature extractor that summarizes high-level characteristics of a fingerprint image. The data is 6-dimensional and it consists of labeled samples corresponding to the genuine (True, label 1) class and the fake (False, label 0) class.

The dataset can be summarized by the image in Figures \ref{fig:dataset}. It shows the data distribution of features and their correlation with each other.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dataset_12.png}
        \caption{Features 1 and 2.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dataset_34.png}
        \caption{Features 3 and 4.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dataset_56.png}
        \caption{Features 5 and 6.}
    \end{subfigure}
    \caption{The figure shows histograms of the projected data on each feature. The scatter plot shows the correlation between the two features.}
    \label{fig:dataset}
\end{figure}

The first two features are mostly overlapping, so it is difficult to separate the two classes. The histograms show that they can be probably well approximated by a Gaussian density function. Further more means are very close to each other, but the variances are different. 

The third and fourth features are more separated, but still overlapping. Also these features can be approximated by a Gaussian density function but the means are different and variance very similar. These features are more useful in a classification task as they are more discriminative.

The last two features are the most discriminative, they can't be approximated by a simple unimodal Gaussian density function because they seems to be bimodal or trimodal. Thanks to this behavior they form 4 distinct clusters (two for each feature) that can be easily separated. They are probably the most useful features if the classifier is able to capture the complex structure.

\section{Lab 3: PCA and LDA}
\label{sec:pca_lda}

\subsection{Dimensionality Reduction}
Principal Component Analysis (PCA) is a dimensionality reduction technique that finds the directions of maximum variance in the data. It projects the data onto a lower-dimensional subspace while preserving as much variance as possible. The Figure \ref{fig:dataset_pca} shows the results of PCA on the dataset. The first principal component captures most of the variance and it separates decently the two classes. The other principal components overlap, so they are not very useful for classification.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/dataset_pca.png}
    \caption{The figure shows the results of PCA on the dataset. The histograms show data distribution on each principal component. The top left one represents the first principal component, the bottom right is the last one. }
    \label{fig:dataset_pca}
\end{figure}

Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that finds the directions that maximize the separation between classes. The Figure \ref{fig:dataset_lda} shows the results of LDA on the dataset. Only one linear discriminant is found because the number of classes is 2. The linear discriminant separates the two classes partially. It is comparable to the first principal component of PCA.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/dataset_lda.png}
    \caption{The figure shows the results of LDA on the dataset. The histogram shows data distribution on the only linear discriminant found.}
    \label{fig:dataset_lda}
\end{figure}

Both cases simplify the dataset as much that the well-distinctive cluster of the last two features is lost. So, the preprocessing techniques cloud be useful to speed up the computation but they could also lose important information. In very simple dataset ad this one, with only 6 dimensions, presumably the best choice is to use the original dataset.

\subsection{Classification}
The project analyses the performance of LDA method as a classifier. In order to get reliable results, the experiments' performance is evaluated with k-fold cross-validation with k equal to 10. It is also inspect the improvement of the classifier by using PCA as a preprocessing technique.

The Table \ref{tab:lda_performance} shows the performance results as error rate, Detection Cost Function (DCF) and Minimum DCF. The last one is useful to understand the gains of changing the threshold to optimal value.

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lXXX}
        \toprule
        \textbf{PCA} & \textbf{Error Rate (\%)} & \textbf{DCF} & \textbf{Min DCF} \\
        \midrule
        None   & 9.58 & 0.192 & 0.176 \\
        1 PC  & 9.68 & 0.194 & 0.177 \\
        2 PC  & 9.63 & 0.193 & 0.178 \\
        3 PC  & 9.43 & 0.189 & 0.176 \\
        4 PC  & 9.57 & 0.192 & 0.175 \\
        5 PC  & 9.58 & 0.192 & 0.175 \\
        6 PC  & 9.62 & 0.193 & 0.176 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for LDA models with various dimensionality reduction techniques.}
    \label{tab:lda_performance}
\end{table}

As intuitively retrieved from PCA and LDA graphs, the capabilities of both methods are similar. In fact, the performance of the classifier is not improved by using PCA as a preprocessing technique. 

In general the performance are terrible considering the simplicity of the dataset. Further more the threshold selection is not optimal, so the performance could be improved by tuning it a little bit.

\section{Lab 4: Multivariate Gaussian density}
\label{sec:gaussian_density}
As preliminary step, the project analyses the correspondence between the features distribution and the Gaussian density function. This is helpful to understand the potential efficacy of model based on Gaussian density.
The figure \ref{fig:dataset_gaussian} shows the histograms of the dataset and the Gaussian density function that best fits the data.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/dataset_gaussian.png}
    \caption{The figure shows the histograms of the dataset and the Gaussian density function that best fits each feature.}
    \label{fig:dataset_gaussian}
\end{figure}

The results are consistent with the previous analysis. The first four features can be approximated by a unimodal Gaussian density function. Instead, the last two can not be well represented thanks to their multimodality.


\section{Lab 5: Multivariate Gaussian Model}

The project analyses the performance of Multivariate Gaussian (MVG) models with different covariance matrices: full, tied, and diagonal. The experiments are conducted with k-fold cross-validation with k equal to 10. The results are shown in Table \ref{tab:mvg_performance}.

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lXXX}
        \toprule
        \textbf{Model} & \textbf{Error Rate (\%)} & \textbf{DCF} & \textbf{Min DCF} \\
        \midrule
        MVG      & 7.45 & 0.15 & 0.131 \\
        TiedMVG  & 9.62 & 0.193 & 0.176 \\
        NaiveMVG & 7.48 & 0.15 & 0.13 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for various MVG models without dimensionality reduction.}
    \label{tab:mvg_performance}
\end{table}

The results show that the tied MVG performs worse than the others. This is due to the fact that the covariance matrix change considerably along the features and so it can't be approximated by a single matrix. To visualize this better, the Figure \ref{fig:dataset_gaussian_tied} shows the estimated Gaussian density function for the tied MVG model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/dataset_gaussian_tied.png}
    \caption{The figure shows the histograms of the dataset and the Gaussian density function with tied covariance matrix for each feature.}
    \label{fig:dataset_gaussian_tied}
\end{figure}

The graphs show that the tied MVG model can practically use only features 3 and 4 as discriminative ones. The other features are approximated with the same gaussian function fro both classes.

\subsection{Correlation Analysis}

In order to analyze further the performance of Naive MVG model respect to the full MVG model, the project computes the pearson correlation coefficient between the features for each class. The results are shown in Figure \ref{fig:dataset_person}. 
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dataset_pearson_false.png}
        \caption{Class False.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dataset_pearson_true.png}
        \caption{Class True.}
    \end{subfigure}
    \caption{The figure shows the Pearson correlation coefficient between the features for each class. Correlated features are close to 1 or -1.}
    \label{fig:dataset_person}
\end{figure}

The graphs show the reason why the Naive MVG model performs as well as the full MVG model. The features are not correlated at all, so the assumption of independence is not violated. 

\subsection{Assumption Analysis}

The Gaussian model assumes that features can be jointly modeled by Gaussian distributions. The good-ness of the model is therefore strongly affected by the accuracy of this assumption. As already discussed, the last two features do not satisfy this assumption. To analyze if indeed the last set of features negatively affects our classifier, we can try repeating the classification using only feature 1 to 4. The results are shown in Table \ref{tab:mvg_performance_4}.

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lXXX}
        \toprule
        \textbf{Model} & \textbf{Error Rate (\%)} & \textbf{DCF} & \textbf{Min DCF} \\
        \midrule
        MVG      & 8.35 & 0.167 & 0.148 \\
        TiedMVG  & 9.73 & 0.195 & 0.175 \\
        NaiveMVG & 8.35 & 0.167 & 0.149 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for various MVG models with only the first 4 features.}
    \label{tab:mvg_performance_4}
\end{table}

For both MVG and Naive MVG models, the performance is slightly worse than the full dataset. This implies that the last two features are useful anyway, even if they are not well approximated by a Gaussian. 

\subsection{Mean and Variance of Features Analysis}

To further explore how means and variance of the features affect the performance of different approaches, the project repeats the classification using only features 1 and 2 and then only features 3 and 4. This is done because features 1 and 2 means are similar but variances are not, whereas for features 3 and 4 the two classes mainly differ for the feature mean, but show similar variance. The results are shown in Table \ref{tab:mvg_performance_12} and Table \ref{tab:mvg_performance_34}.

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lXXX}
        \toprule
        \textbf{Model} & \textbf{Error Rate (\%)} & \textbf{DCF} & \textbf{Min DCF} \\
        \midrule
        MVG      & 36.83 & 0.738 & 0.454 \\
        TiedMVG  & 49.55 & 0.99  & 0.488 \\
        NaiveMVG & 36.80 & 0.737 & 0.454 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for various MVG models with only the first 2 features.}
    \label{tab:mvg_performance_12}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lXXX}
        \toprule
        \textbf{Model} & \textbf{Error Rate (\%)} & \textbf{DCF} & \textbf{Min DCF} \\
        \midrule
        MVG      & 9.67 & 0.194 & 0.175 \\
        TiedMVG  & 9.68 & 0.194 & 0.175 \\
        NaiveMVG & 9.68 & 0.194 & 0.175 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for various MVG models with only the 3 and 4 features.}
    \label{tab:mvg_performance_34}
\end{table}

Lets first analyze the generale performance between the two cases. The performance of the first two features is terrible, this is due to the fact that the two classes are not separable at all and this shows how important is the mean of class distribution. Instead, the performance of features 3 and 4 is very good, this is due to the fact that the two classes mean are very different and this remark how much these features are discriminative.

More over, in the first case Tied MVG model performs even worse because the features' variances are unsimilar. Instead in the second case, the performances are the same for all models because the features' variances are pretty the same.

\subsection{Dimensionality reduction}
Finally the project investigates the performance of MVG models with PCA as a preprocessing technique. The results are shown in Table \ref{tab:mvg_performance_pca}.

\begin{table}[ht]
    \centering
    \begin{tabularx}{\textwidth}{ll*{6}{X}}
        \toprule
        \textbf{Model} & \textbf{Metrics} & \textbf{PCA 1} & \textbf{PCA 2} & \textbf{PCA 3} & \textbf{PCA 4} & \textbf{PCA 5} & \textbf{PCA 6} \\
        \midrule
        \multirow{3}{*}{MVG} & Error & 9.7 & 9.23 & 8.78 & 8.33 & 7.38 & 7.45 \\
                              & DCF & 0.195 & 0.185 & 0.176 & 0.167 & 0.148 & 0.15 \\
                              & Min DCF & 0.177 & 0.171 & 0.159 & 0.149 & 0.131 & 0.131 \\
        \midrule
        \multirow{3}{*}{TiedMVG} & Error Rate & 9.68 & 9.63 & 9.45 & 9.58 & 9.57 & 9.62 \\
                                 & DCF & 0.194 & 0.193 & 0.19 & 0.192 & 0.192 & 0.193 \\
                                 & Min DCF & 0.177 & 0.178 & 0.176 & 0.175 & 0.175 & 0.176 \\
        \midrule
        \multirow{3}{*}{NaiveMVG} & Error Rate & 9.7 & 9.32 & 9.3 & 9.02 & 8.58 & 8.6 \\
                                  & DCF & 0.195 & 0.187 & 0.186 & 0.181 & 0.172 & 0.172 \\
                                  & Min DCF & 0.177 & 0.171 & 0.162 & 0.159 & 0.154 & 0.154 \\
        \bottomrule
    \end{tabularx}
    \caption{Performance metrics for various MVG models with PCA as a preprocessing technique.}
    \label{tab:mvg_performance_pca}
\end{table}

The results confirm the hypothesis done in Dimensionality Reduction section. The performance of the classifier is not improved by using PCA as a preprocessing technique due to the lostness of the information. Indeed the best results are obtained with higher number of principal components. Tied MVG model is the only one that performs the same with and without PCA, this is due to PCA that provides features with similar variances.

\subsection{Evaluation Metrics}
Explain the metrics used to evaluate the performance of your models.

\section{Experiments}
\label{sec:experiments}
Describe the experiments conducted to validate your models. Include details such as experimental setup, datasets used, and any specific configurations.

\section{Results}
\label{sec:results}
Present the results of your experiments. Use tables, figures, and charts to illustrate the performance of your models.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model & Accuracy & F1 Score \\
        \hline
        Model 1 & 0.90 & 0.88 \\
        Model 2 & 0.85 & 0.84 \\
        Model 3 & 0.92 & 0.90 \\
        \hline
    \end{tabular}
    \caption{Model performance comparison}
    \label{tab:results}
\end{table}

\section{Discussion}
\label{sec:discussion}
Interpret the results. Discuss any patterns, insights, or anomalies observed. Compare the performance of different models and explain any differences.

\section{Conclusion}
\label{sec:conclusion}
Summarize the findings of your project. Discuss the implications of your results, any limitations of your study, and possible future work.

\end{document}
